In Progress

# Academic Citation Crawler & Dashboard

A distributed crawler for harvesting academic citation data from Semantic Scholar and visualizing system stats via a live dashboard. Designed for robustness, performance, and extensibility.

---

## ğŸ“š Table of Contents

- [Overview](#overview)
- [Technologies Used](#technologies-used)
- [Architecture](#architecture)
- [Features](#features)
- [How to Run](#how-to-run)
- [API & Interfaces](#api--interfaces)
- [Challenges](#challenges)
- [Crawler Script (`crawler.py`)](#crawlerpy)
- [Dashboard Script (`dashboard.py`)](#dashboardpy)

---

## ğŸ“Œ Overview

This project implements a robust academic crawler to collect citation data via the [Semantic Scholar API](https://api.semanticscholar.org/). The data is processed, stored in PostgreSQL, and de-duplicated using Redis with a Bloom filter. A live dashboard (FastAPI) monitors crawler performance and remote server resources.

---

## ğŸ› ï¸ Technologies Used

- **Python 3**
- **PostgreSQL** (`psycopg2`, `asyncpg`)
- **Redis** + Bloom Filter
- **FastAPI** (Dashboard backend)
- **asyncssh** (RAM monitoring via SSH)
- **Semantic Scholar API**
- **Tenacity** (for robust retry logic)
- **Uvicorn** (for serving dashboard)

---

## ğŸ—ï¸ Architecture

                   +-----------------------+
                   |  Semantic Scholar API |
                   +----------+------------+
                              |
                              v
                   +-----------------------+
                   |      Crawler.py       |
                   |-----------------------|
                   |  - Batched Fetching   |
                   |  - Reference Parsing  |
                   |  - Deduplication      |
                   |  - Queue Handling     |
                   +----------+------------+
                              |
               +--------------+--------------+
               |                             |
               v                             v
      +-------------------+       +------------------------+
      |     PostgreSQL    |       |         Redis          |
      |-------------------|       |------------------------|
      |  - Processed IDs  |       |  - Task Queue (FIFO)   |
      |  - Citations      |       |  - Bloom Filter        |
      +-------------------+       +------------------------+
               |
               v
     +--------------------------+
     |      FastAPI Dashboard   |
     |--------------------------|
     | - Crawl Stats            |
     | - Crawl Speed            |
     | - Remote RAM Monitoring  |
     +--------------------------+
               |
               v
     +--------------------------+
     |     Web Browser UI       |
     +--------------------------+

---

## âœ¨ Features

- âœ… Distributed crawling with queue-based task management
- ğŸ§  Deduplication with Redis Bloom filters + SQL fallback
- ğŸ” Retry logic with exponential backoff
- ğŸ•¸ï¸ Citation graph generation (directed edges)
- ğŸ“Š Real-time dashboard with memory, rate, and paper stats
- ğŸ“¦ Background memory checks on remote servers via SSH

---

## ğŸ§ª crawler.py

The main script that handles fetching, deduplication, citation parsing, and task queue management.

### ğŸ§© Key Components

- send_request: Rate-limited API requests with retry logic
- fetch_references_paginated: Handles large reference sets (>1000)
- filter_new_ids: Bloom filter + SQL fallback deduplication
- safe_insert_citations: Robust insert with deadlock handling
- mark_processed: Marks paper as crawled in both Redis and SQL
- main(): Main crawl loop with seed support, resume, and batching

---

## ğŸ“Š dashboard.py

A FastAPI app providing real-time monitoring for crawler performance and system resource usage.

### ğŸ“¡ Endpoints

- GET - HTML Dashboard UI
- GET /status - Returns crawler metrics as JSON
  
### ğŸ“ˆ Metrics Shown

- âœ… Number of processed papers
- ğŸ”— Citation edges discovered
- ğŸ§  Memory pressure (macOS local)
- ğŸ§  RAM usage on remote servers via SSH
- âš¡ Papers/second & ğŸ“ˆ papers/hour
- ğŸ•’ Estimated time per 1000 papers

### ğŸ§µ Background Tasks

- remote_ram_background_updater() â€“ polls RAM usage every 60s
- speed_background_updater() â€“ updates crawl rate every 15s

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ crawler.py
â”œâ”€â”€ dashboard.py
â””â”€â”€ requirements.txt
```
---

## â–¶ï¸ How to Run

### ğŸ”§ Requirements

- PostgreSQL and Redis running locally
- Semantic Scholar API Key (`API_KEY`)
- Python packages: see `requirements.txt`

### ğŸš€ Commands

Start a **fresh** crawl with seed IDs:
```bash
python crawler.py --fresh <seed_id1> <seed_id2>
```
Resume a previously interrupted crawl:
```bash
python crawler.py --resume
```
Run the **Dashboard** server:
```bash
uvicorn dashboard:app --reload
```

---

## ğŸ”Œ API & Interfaces

### ğŸ“¡ Semantic Scholar API
Base URL: https://api.semanticscholar.org/graph/v1
- GET /paper/{id}/references: For paginated citation lists
- POST /paper/batch: For batched paper metadata

### ğŸ” Redis
- paper_queue	- FIFO queue of papers to crawl
- processed_bloom	- RedisBloom filter to avoid duplicate paper IDs

### ğŸ—ƒï¸ PostgreSQL
- processed_papers	Stores paper_id and its fields_of_study
- citations	Stores directed edges (citing_id â†’ cited_id)

### ğŸŒ FastAPI Dashboard
- GET - HTML dashboard UI
- GET /status -	Returns system stats in JSON (see below)

---
